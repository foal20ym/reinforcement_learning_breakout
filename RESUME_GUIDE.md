# üöÄ Resuming Training & Speed Optimization Guide

## Resuming Training from Checkpoint

### Quick Resume (Recommended)

Simply run the training command - it will auto-detect your latest checkpoint:

```bash
python main.py --train
# Or
python train.py
```

You'll see:
```
Found existing checkpoint: checkpoints/dqn_breakout_episode_7200.pt
Resume from latest checkpoint? [Y/n]:
```

Press **Enter** or type **y** to resume!

### Resume from Specific Checkpoint

```bash
python main.py --resume checkpoints/dqn_breakout_episode_7200.pt
# Or
python train.py --resume checkpoints/dqn_breakout_episode_7200.pt
```

### Resume from Latest (No Prompt)

```bash
python main.py --resume latest
# Or
python train.py --resume latest
```

### Start Fresh (Ignore Checkpoints)

```bash
python main.py --fresh
# Or
python train.py --fresh
```

## What Gets Restored?

When you resume from a checkpoint, the following are restored:

‚úÖ **Network weights** (policy and target networks)
‚úÖ **Optimizer state** (Adam momentum, learning rates)
‚úÖ **Training step count** (for proper epsilon decay)
‚úÖ **Epsilon value** (exploration rate)
‚úÖ **Training statistics** (rewards, losses, plots)
‚úÖ **Episode counter** (continues from where you left off)

Your agent will continue exactly where it stopped!

## Speed Optimization Guide

### 1. Use Fast Config (Recommended)

Create a fast training script or modify your imports:

```python
# In train.py, change:
from utils import config_breakout as config
# To:
from utils import config_fast as config
```

**What this changes:**
- Larger batch size (32‚Üí64): Better GPU utilization
- Start learning sooner (10k‚Üí5k steps): Less waiting
- Faster exploration decay (1M‚Üí750k steps): Exploit sooner
- Slightly higher learning rate (0.00025‚Üí0.0003): Learn faster

**Expected speedup:** ~30-40% faster

### 2. Optimize Your Environment

Check your GPU usage:

```bash
# Monitor GPU in another terminal
watch -n 1 nvidia-smi
```

If GPU utilization is low (<50%), try:

```python
# In utils/config_breakout.py or config_fast.py
BATCH_SIZE = 128  # Even larger batches
```

### 3. Use Multiple CPU Cores

If you have a powerful CPU, you can process frames faster:

```bash
# Set environment variable before training
export OMP_NUM_THREADS=8
python main.py --train
```

### 4. Reduce Checkpoint Frequency

```python
# In config
SAVE_FREQ = 200  # Save every 200 episodes instead of 100
LOG_FREQ = 20    # Log less frequently
```

This reduces I/O overhead during training.

### 5. Compile Your Model (PyTorch 2.0+)

If you have PyTorch 2.0+, add this to `core/agent.py`:

```python
# After creating policy_net
self.policy_net = torch.compile(self.policy_net)
self.target_net = torch.compile(self.target_net)
```

**Expected speedup:** 10-20% on compatible hardware

### 6. Profile Your Training

Find bottlenecks:

```bash
# Run with profiling
python -m cProfile -o profile.stats train.py --resume latest

# Analyze results
python -m pstats profile.stats
> sort cumulative
> stats 20
```

## Speed Comparison

| Configuration | Episodes/Hour (GPU) | Episodes/Hour (CPU) |
| ------------- | ------------------- | ------------------- |
| Default       | ~40-50              | ~5-10               |
| Fast Config   | ~55-70              | ~7-12               |
| + Compiled    | ~60-80              | ~7-12               |
| + Optimized   | ~70-90              | ~8-15               |

*Estimated on RTX 3060 / Ryzen 5600X*

## Training Tips

### Monitor Your Progress

```bash
# Check latest checkpoint
ls -lht checkpoints/ | head

# View training plot
eog logs/training_plot.png  # or use your image viewer
```

### Adjust on the Fly

If training seems too slow or unstable:

1. **Stop training** (Ctrl+C)
2. **Edit config** (`utils/config_breakout.py` or `config_fast.py`)
3. **Resume training** (`python main.py --train`)

The new config will apply from the resumed episode!

### Best Practices

‚úÖ **Do:**
- Resume from latest checkpoint after interruptions
- Use fast config if you have good GPU
- Monitor GPU utilization
- Check training plots regularly

‚ùå **Don't:**
- Change network architecture mid-training
- Switch between configs frequently
- Use batch sizes that OOM your GPU
- Train on CPU if you have GPU available

## Performance Without Hurting Agent

These optimizations are **safe** and won't hurt performance:

1. ‚úÖ Larger batch size (32‚Üí64 or 128)
2. ‚úÖ Model compilation (PyTorch 2.0+)
3. ‚úÖ Reduce checkpoint frequency
4. ‚úÖ Multi-threading for environment

These are **slightly aggressive** but still good:

1. ‚ö†Ô∏è Start learning sooner (10k‚Üí5k)
2. ‚ö†Ô∏è Faster epsilon decay (1M‚Üí750k)
3. ‚ö†Ô∏è Higher learning rate (0.00025‚Üí0.0003)

These might **hurt performance**:

1. ‚ùå Very high learning rate (>0.001)
2. ‚ùå Very small batch size (<16)
3. ‚ùå Skip target updates (frequency <500)
4. ‚ùå Start learning too early (<1000 steps)

## Quick Commands Reference

```bash
# Resume from latest (auto-detect)
python main.py --train

# Resume from specific checkpoint
python main.py --resume checkpoints/dqn_breakout_episode_7200.pt

# Resume from latest (no prompt)
python main.py --resume latest

# Start completely fresh
python main.py --fresh

# Evaluate current best
python main.py --evaluate --checkpoint checkpoints/dqn_breakout_episode_7200.pt
```

## Troubleshooting

**Q: Training doesn't resume, starts from episode 1**
A: Make sure you're using the updated `train.py` with resume support

**Q: "Checkpoint not found" error**
A: Check that the file exists: `ls checkpoints/`

**Q: GPU out of memory after changing batch size**
A: Reduce `BATCH_SIZE` in config, or reduce `REPLAY_BUFFER_SIZE`

**Q: Training is slower after resume**
A: Normal - epsilon is lower, so less random exploration (more computation)

**Q: Can I switch between configs?**
A: Yes! Just change the import in `train.py` and resume

---

You're currently at **episode 7200** - great progress! üéâ

Resume with: `python main.py --train` or `python main.py --resume latest`
